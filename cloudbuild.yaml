# CIPHER Platform - Production Cloud Build Configuration
# Service Account: cloud-build-service@primal-chariot-382610.iam.gserviceaccount.com

timeout: 1800s  # 30 minutes timeout for complete deployment

steps:
  # Step 1: Validate environment and prerequisites
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "🛡️ CIPHER Platform - Production Deployment Starting"
        echo "=============================================="
        echo "Project ID: $PROJECT_ID"
        echo "Build ID: $BUILD_ID"
        echo "Service Account: cloud-build-service@$PROJECT_ID.iam.gserviceaccount.com"
        echo "Timestamp: $$(date)"
        echo ""
        
        # Validate required APIs are enabled
        echo "🔍 Validating required APIs..."
        gcloud services list --enabled --filter="name:(run.googleapis.com OR cloudbuild.googleapis.com OR bigquery.googleapis.com)" --format="value(name)" | wc -l
        
        # Check service account permissions
        echo "🔑 Validating service account permissions..."
        gcloud projects get-iam-policy $PROJECT_ID \
          --flatten="bindings[].members" \
          --filter="bindings.members:cloud-build-service@$PROJECT_ID.iam.gserviceaccount.com" \
          --format="value(bindings.role)" | wc -l
        
        echo "✅ Pre-deployment validation completed"
    id: 'validate'

  # Step 2: Setup BigQuery infrastructure with enhanced schema migration
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "📊 Setting up BigQuery infrastructure with enhanced schema..."
        
        # Create dataset if not exists
        if ! bq ls --project_id=$PROJECT_ID telegram_data >/dev/null 2>&1; then
          echo "Creating BigQuery dataset: telegram_data"
          bq mk --dataset \
            --location=US \
            --description="CIPHER Cybersecurity Intelligence Platform Data" \
            $PROJECT_ID:telegram_data
        else
          echo "✅ BigQuery dataset telegram_data already exists"
        fi
        
        # Check if table exists and get current schema
        TABLE_EXISTS=false
        CURRENT_FIELDS=0
        if bq ls --project_id=$PROJECT_ID telegram_data.processed_messages >/dev/null 2>&1; then
          TABLE_EXISTS=true
          CURRENT_FIELDS=$$(bq show --format=json $PROJECT_ID:telegram_data.processed_messages | jq '.schema.fields | length')
          echo "✅ Table exists with $$CURRENT_FIELDS fields"
        fi
        
        # Create comprehensive enhanced schema
        cat > /tmp/cipher_enhanced_schema.json << 'EOF'
        [
          {"name": "message_id", "type": "STRING", "mode": "REQUIRED", "description": "Unique message identifier"},
          {"name": "chat_id", "type": "STRING", "mode": "REQUIRED", "description": "Telegram chat/channel ID"},
          {"name": "chat_username", "type": "STRING", "mode": "NULLABLE", "description": "Channel username (e.g., @DarkfeedNews)"},
          {"name": "user_id", "type": "STRING", "mode": "NULLABLE", "description": "Telegram user ID"},
          {"name": "username", "type": "STRING", "mode": "NULLABLE", "description": "Username without @ symbol"},
          {"name": "message_text", "type": "STRING", "mode": "NULLABLE", "description": "Original message content"},
          {"name": "message_date", "type": "TIMESTAMP", "mode": "REQUIRED", "description": "When message was sent"},
          {"name": "processed_date", "type": "TIMESTAMP", "mode": "REQUIRED", "description": "When message was processed"},
          
          {"name": "gemini_analysis", "type": "STRING", "mode": "NULLABLE", "description": "Gemini AI comprehensive threat analysis"},
          {"name": "sentiment", "type": "STRING", "mode": "NULLABLE", "description": "Message sentiment: positive/negative/neutral"},
          {"name": "confidence_score", "type": "FLOAT", "mode": "NULLABLE", "description": "AI analysis confidence (0.0-1.0)"},
          {"name": "key_topics", "type": "STRING", "mode": "REPEATED", "description": "Key cybersecurity topics identified"},
          {"name": "urgency_score", "type": "FLOAT", "mode": "NULLABLE", "description": "Threat urgency score (0.0-1.0)"},
          {"name": "category", "type": "STRING", "mode": "NULLABLE", "description": "Primary threat category"},
          {"name": "subcategory", "type": "STRING", "mode": "NULLABLE", "description": "Specific threat subcategory"},
          
          {"name": "threat_level", "type": "STRING", "mode": "NULLABLE", "description": "Threat level: critical/high/medium/low/info"},
          {"name": "threat_type", "type": "STRING", "mode": "NULLABLE", "description": "Specific threat type (e.g., APT, ransomware)"},
          {"name": "attack_stage", "type": "STRING", "mode": "NULLABLE", "description": "Attack lifecycle stage"},
          {"name": "kill_chain_phase", "type": "STRING", "mode": "NULLABLE", "description": "MITRE ATT&CK kill chain phase"},
          
          {"name": "channel_type", "type": "STRING", "mode": "NULLABLE", "description": "Source channel type"},
          {"name": "channel_priority", "type": "STRING", "mode": "NULLABLE", "description": "Channel priority level"},
          {"name": "channel_focus", "type": "STRING", "mode": "NULLABLE", "description": "Channel focus area"},
          
          {"name": "iocs_detected", "type": "STRING", "mode": "REPEATED", "description": "All IOCs found"},
          {"name": "ip_addresses", "type": "STRING", "mode": "REPEATED", "description": "IP addresses found"},
          {"name": "domains", "type": "STRING", "mode": "REPEATED", "description": "Domains found"},
          {"name": "urls", "type": "STRING", "mode": "REPEATED", "description": "URLs found"},
          {"name": "file_hashes", "type": "STRING", "mode": "REPEATED", "description": "File hashes (MD5, SHA1, SHA256)"},
          {"name": "email_addresses", "type": "STRING", "mode": "REPEATED", "description": "Email addresses found"},
          
          {"name": "cve_references", "type": "STRING", "mode": "REPEATED", "description": "CVE references mentioned"},
          {"name": "cwe_references", "type": "STRING", "mode": "REPEATED", "description": "CWE references mentioned"},
          {"name": "mitre_techniques", "type": "STRING", "mode": "REPEATED", "description": "MITRE ATT&CK techniques"},
          {"name": "malware_families", "type": "STRING", "mode": "REPEATED", "description": "Malware families identified"},
          {"name": "threat_actors", "type": "STRING", "mode": "REPEATED", "description": "Threat actors/groups mentioned"},
          {"name": "campaign_names", "type": "STRING", "mode": "REPEATED", "description": "Campaign or operation names"},
          
          {"name": "affected_systems", "type": "STRING", "mode": "REPEATED", "description": "Systems/platforms affected"},
          {"name": "affected_vendors", "type": "STRING", "mode": "REPEATED", "description": "Vendors/companies affected"},
          {"name": "attack_vectors", "type": "STRING", "mode": "REPEATED", "description": "Attack vectors mentioned"},
          {"name": "vulnerabilities", "type": "STRING", "mode": "REPEATED", "description": "Vulnerability types"},
          {"name": "geographical_targets", "type": "STRING", "mode": "REPEATED", "description": "Geographic regions targeted"},
          {"name": "industry_targets", "type": "STRING", "mode": "REPEATED", "description": "Industries targeted"},
          
          {"name": "source_reliability", "type": "STRING", "mode": "NULLABLE", "description": "Source reliability assessment"},
          {"name": "information_type", "type": "STRING", "mode": "NULLABLE", "description": "Type of intelligence information"},
          {"name": "sharing_level", "type": "STRING", "mode": "NULLABLE", "description": "Information sharing level (TLP)"},
          {"name": "tags", "type": "STRING", "mode": "REPEATED", "description": "Custom tags for categorization"},
          
          {"name": "processing_time_ms", "type": "INTEGER", "mode": "NULLABLE", "description": "Processing time in milliseconds"},
          {"name": "data_quality_score", "type": "FLOAT", "mode": "NULLABLE", "description": "Data quality assessment (0.0-1.0)"},
          {"name": "false_positive_risk", "type": "STRING", "mode": "NULLABLE", "description": "False positive risk assessment"}
        ]
        EOF
        
        TARGET_FIELDS=$$(cat /tmp/cipher_enhanced_schema.json | jq '. | length')
        echo "Target schema has $$TARGET_FIELDS fields"
        
        if [ "$$TABLE_EXISTS" = "true" ]; then
          if [ "$$CURRENT_FIELDS" -lt "$$TARGET_FIELDS" ]; then
            echo "🔄 Upgrading table schema from $$CURRENT_FIELDS to $$TARGET_FIELDS fields"
            
            # Create new table with enhanced schema (BigQuery doesn't support adding many fields at once)
            echo "Creating backup and new table with enhanced schema..."
            
            # Backup existing data
            bq cp $PROJECT_ID:telegram_data.processed_messages $PROJECT_ID:telegram_data.processed_messages_backup_$$(date +%Y%m%d_%H%M%S)
            
            # Create new table with enhanced schema
            bq rm -f $PROJECT_ID:telegram_data.processed_messages_new
            bq mk --table \
              --description="CIPHER Cybersecurity Intelligence Messages - Enhanced Schema" \
              --time_partitioning_field=processed_date \
              --time_partitioning_type=DAY \
              --clustering_fields=threat_level,channel_type,category,threat_type \
              $PROJECT_ID:telegram_data.processed_messages_new \
              /tmp/cipher_enhanced_schema.json
            
            # Copy existing data to new table (only matching fields)
            echo "Migrating existing data to enhanced schema..."
            bq query --use_legacy_sql=false --destination_table=$PROJECT_ID:telegram_data.processed_messages_new --append_table \
            "SELECT 
              message_id,
              chat_id,
              COALESCE(chat_username, '') as chat_username,
              COALESCE(user_id, '') as user_id,
              COALESCE(username, '') as username,
              COALESCE(message_text, '') as message_text,
              message_date,
              processed_date,
              COALESCE(gemini_analysis, '') as gemini_analysis,
              COALESCE(sentiment, 'neutral') as sentiment,
              NULL as confidence_score,
              COALESCE(key_topics, []) as key_topics,
              COALESCE(urgency_score, 0.0) as urgency_score,
              COALESCE(category, 'other') as category,
              NULL as subcategory,
              COALESCE(threat_level, 'low') as threat_level,
              COALESCE(threat_type, 'unknown') as threat_type,
              NULL as attack_stage,
              NULL as kill_chain_phase,
              COALESCE(channel_type, 'unknown') as channel_type,
              COALESCE(channel_priority, 'medium') as channel_priority,
              COALESCE(channel_focus, 'general') as channel_focus,
              COALESCE(iocs_detected, []) as iocs_detected,
              [] as ip_addresses,
              [] as domains,
              [] as urls,
              [] as file_hashes,
              [] as email_addresses,
              COALESCE(cve_references, []) as cve_references,
              [] as cwe_references,
              [] as mitre_techniques,
              COALESCE(malware_families, []) as malware_families,
              COALESCE(threat_actors, []) as threat_actors,
              [] as campaign_names,
              COALESCE(affected_systems, []) as affected_systems,
              [] as affected_vendors,
              COALESCE(attack_vectors, []) as attack_vectors,
              COALESCE(vulnerabilities, []) as vulnerabilities,
              COALESCE(geographical_targets, []) as geographical_targets,
              COALESCE(industry_targets, []) as industry_targets,
              'C' as source_reliability,
              'tactical' as information_type,
              'TLP:GREEN' as sharing_level,
              COALESCE(tags, []) as tags,
              NULL as processing_time_ms,
              NULL as data_quality_score,
              'medium' as false_positive_risk
            FROM \`$PROJECT_ID.telegram_data.processed_messages\`"
            
            # Replace old table with new one
            bq rm -f $PROJECT_ID:telegram_data.processed_messages
            bq cp $PROJECT_ID:telegram_data.processed_messages_new $PROJECT_ID:telegram_data.processed_messages
            bq rm -f $PROJECT_ID:telegram_data.processed_messages_new
            
            echo "✅ Table schema upgraded successfully"
          else
            echo "✅ Table schema is up to date ($$CURRENT_FIELDS fields)"
          fi
        else
          echo "Creating new enhanced BigQuery table..."
          bq mk --table \
            --description="CIPHER Cybersecurity Intelligence Messages - Enhanced Schema" \
            --time_partitioning_field=processed_date \
            --time_partitioning_type=DAY \
            --clustering_fields=threat_level,channel_type,category,threat_type \
            $PROJECT_ID:telegram_data.processed_messages \
            /tmp/cipher_enhanced_schema.json
          echo "✅ Created enhanced BigQuery table with $$TARGET_FIELDS fields"
        fi
        
        # Verify final schema
        FINAL_FIELDS=$$(bq show --format=json $PROJECT_ID:telegram_data.processed_messages | jq '.schema.fields | length')
        echo "✅ Final table has $$FINAL_FIELDS fields"
        
        # Cleanup
        rm -f /tmp/cipher_enhanced_schema.json
        
        echo "✅ Enhanced BigQuery infrastructure setup completed"
    id: 'setup-bigquery'
    waitFor: ['validate']

  # Step 3: Build Docker image
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'build',
      '--tag', 'gcr.io/$PROJECT_ID/telegram-ai-processor:$BUILD_ID',
      '--tag', 'gcr.io/$PROJECT_ID/telegram-ai-processor:latest',
      '--build-arg', 'PROJECT_ID=$PROJECT_ID',
      '--build-arg', 'BUILD_ID=$BUILD_ID',
      '.'
    ]
    id: 'build-image'
    waitFor: ['setup-bigquery']

  # Step 4: Push Docker image to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', '--all-tags', 'gcr.io/$PROJECT_ID/telegram-ai-processor']
    id: 'push-image'
    waitFor: ['build-image']

  # Step 5: Deploy to Cloud Run with production configuration
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'gcloud'
    args: [
      'run', 'deploy', 'telegram-ai-processor',
      '--image', 'gcr.io/$PROJECT_ID/telegram-ai-processor:$BUILD_ID',
      '--region', 'us-central1',
      '--platform', 'managed',
      '--allow-unauthenticated',
      '--service-account', 'cloud-build-service@$PROJECT_ID.iam.gserviceaccount.com',
      '--memory', '4Gi',
      '--cpu', '2',
      '--port', '8080',
      '--timeout', '3600',
      '--concurrency', '80',
      '--min-instances', '0',
      '--max-instances', '10',
      '--set-env-vars', 'GOOGLE_CLOUD_PROJECT=$PROJECT_ID,LOG_LEVEL=INFO,DATASET_ID=telegram_data,TABLE_ID=processed_messages,PYTHONUNBUFFERED=1',
      '--labels', 'app=cipher,environment=production,version=$BUILD_ID',
      '--no-cpu-throttling'
    ]
    id: 'deploy'
    waitFor: ['push-image']

  # Step 6: Configure IAM policies
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "⚙️ Configuring IAM policies..."
        
        # Set IAM policies for public access
        echo "Setting IAM policies for public access..."
        gcloud run services add-iam-policy-binding telegram-ai-processor \
          --region=us-central1 \
          --member="allUsers" \
          --role="roles/run.invoker"
        
        echo "✅ IAM configuration applied"
    id: 'configure'
    waitFor: ['deploy']

  # Step 7: Run health checks and verify deployment
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "🏥 Running deployment health checks..."
        
        # Get service URL
        DEPLOYED_URL=$$(gcloud run services describe telegram-ai-processor \
          --region=us-central1 \
          --project=$PROJECT_ID \
          --format='value(status.url)')
        
        if [ -z "$$DEPLOYED_URL" ]; then
          echo "❌ Failed to get service URL"
          exit 1
        fi
        
        echo "Service deployed at: $$DEPLOYED_URL"
        
        # Wait for service to be ready
        echo "Waiting for service to initialize..."
        sleep 45
        
        # Test health endpoints with retries
        echo "Testing health endpoints..."
        
        # Test liveness probe
        echo "Testing /health/live endpoint..."
        for i in {1..5}; do
          HEALTH_STATUS=$$(curl -s -o /dev/null -w "%{http_code}" "$$DEPLOYED_URL/health/live" || echo "000")
          if [ "$$HEALTH_STATUS" = "200" ]; then
            echo "✅ Liveness check: PASSED"
            break
          else
            echo "⚠️ Liveness check attempt $$i: HTTP $$HEALTH_STATUS"
            sleep 10
          fi
        done
        
        # Test readiness probe
        echo "Testing /health endpoint..."
        for i in {1..3}; do
          READY_STATUS=$$(curl -s -o /dev/null -w "%{http_code}" "$$DEPLOYED_URL/health" || echo "000")
          if [ "$$READY_STATUS" = "200" ]; then
            echo "✅ Readiness check: PASSED"
            break
          else
            echo "⚠️ Readiness check attempt $$i: HTTP $$READY_STATUS"
            sleep 15
          fi
        done
        
        # Test main dashboard
        echo "Testing main dashboard..."
        DASH_STATUS=$$(curl -s -o /dev/null -w "%{http_code}" "$$DEPLOYED_URL/dashboard" || echo "000")
        if [ "$$DASH_STATUS" = "200" ]; then
          echo "✅ Dashboard: PASSED"
        else
          echo "⚠️ Dashboard: HTTP $$DASH_STATUS"
        fi
        
        # Test API endpoints with defensive handling
        echo "Testing fixed API endpoints..."
        
        # Test stats API (this was failing before)
        for i in {1..3}; do
          API_STATUS=$$(curl -s -o /dev/null -w "%{http_code}" "$$DEPLOYED_URL/api/stats" || echo "000")
          if [ "$$API_STATUS" = "200" ]; then
            echo "✅ Stats API: FIXED AND WORKING"
            break
          else
            echo "⚠️ Stats API attempt $$i: HTTP $$API_STATUS"
            sleep 10
          fi
        done
        
        # Test insights API
        INSIGHTS_STATUS=$$(curl -s -o /dev/null -w "%{http_code}" "$$DEPLOYED_URL/api/insights" || echo "000")
        if [ "$$INSIGHTS_STATUS" = "200" ]; then
          echo "✅ Insights API: WORKING"
        else
          echo "⚠️ Insights API: HTTP $$INSIGHTS_STATUS"
        fi
        
        # Test analytics API
        ANALYTICS_STATUS=$$(curl -s -o /dev/null -w "%{http_code}" "$$DEPLOYED_URL/api/analytics" || echo "000")
        if [ "$$ANALYTICS_STATUS" = "200" ]; then
          echo "✅ Analytics API: WORKING"
        else
          echo "⚠️ Analytics API: HTTP $$ANALYTICS_STATUS"
        fi
        
        echo ""
        echo "🎉 CIPHER Platform Deployment Completed!"
        echo "======================================="
        echo "Service URL: $$DEPLOYED_URL"
        echo "Dashboard: $$DEPLOYED_URL/dashboard"
        echo "Health Check: $$DEPLOYED_URL/health"
        echo "Stats API: $$DEPLOYED_URL/api/stats"
        echo "Monitoring: $$DEPLOYED_URL/api/monitoring/status"
        echo ""
        echo "🔧 Technical Details:"
        echo "Project: $PROJECT_ID"
        echo "Build ID: $BUILD_ID"
        echo "Service Account: cloud-build-service@$PROJECT_ID.iam.gserviceaccount.com"
        echo "Region: us-central1"
        echo "Memory: 4Gi"
        echo "CPU: 2"
        echo "BigQuery Dataset: telegram_data (ENHANCED SCHEMA)"
        echo ""
        echo "📊 Schema Status:"
        FINAL_FIELDS=$$(bq show --format=json $PROJECT_ID:telegram_data.processed_messages | jq '.schema.fields | length')
        echo "BigQuery Table Fields: $$FINAL_FIELDS"
        echo "Schema Migration: COMPLETED"
        echo ""
        echo "📡 Monitoring Channels:"
        echo "🔴 @DarkfeedNews - Threat Intelligence"
        echo "🟠 @breachdetector - Data Breach Monitor"  
        echo "🔵 @secharvester - Security News"
        echo ""
        echo "🛡️ CIPHER Platform is now operational with FIXED analytics!"
    id: 'verify'
    waitFor: ['configure']

  # Step 8: Generate deployment report
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "📋 Generating enhanced deployment report..."
        
        # Service status
        echo ""
        echo "🔍 Service Status:"
        gcloud run services describe telegram-ai-processor \
          --region=us-central1 \
          --project=$PROJECT_ID \
          --format='table(status.conditions[].type,status.conditions[].status,status.conditions[].reason)'
        
        # Service configuration
        echo ""
        echo "⚙️ Service Configuration:"
        gcloud run services describe telegram-ai-processor \
          --region=us-central1 \
          --project=$PROJECT_ID \
          --format='table(spec.template.spec.containers[].resources.limits.memory,spec.template.spec.containers[].resources.limits.cpu,spec.template.spec.serviceAccountName)'
        
        # Enhanced BigQuery dataset status
        echo ""
        echo "📊 BigQuery Enhanced Status:"
        bq show --format=prettyjson $PROJECT_ID:telegram_data | grep -E '"datasetId"|"location"|"description"' || true
        
        # Table schema verification
        echo ""
        echo "🗃️ Table Schema Status:"
        FIELD_COUNT=$$(bq show --format=json $PROJECT_ID:telegram_data.processed_messages | jq '.schema.fields | length')
        echo "Total Fields: $$FIELD_COUNT"
        echo "Partitioning: DAY (processed_date)"
        echo "Clustering: threat_level, channel_type, category, threat_type"
        
        # Show sample of new fields
        echo ""
        echo "🔍 Enhanced Schema Fields (sample):"
        bq show --format=json $PROJECT_ID:telegram_data.processed_messages | jq -r '.schema.fields[] | select(.name | contains("confidence") or contains("subcategory") or contains("attack_stage")) | "\(.name): \(.type)"' || echo "Enhanced fields verified"
        
        echo ""
        echo "✅ Enhanced deployment report generated successfully"
        echo "📝 Build completed with BigQuery schema migration at: $$(date)"
    id: 'report'
    waitFor: ['verify']

# Required options for service account builds
options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'E2_HIGHCPU_8'
  diskSizeGb: 100
  substitution_option: 'ALLOW_LOOSE'

# Images to be pushed to Container Registry
images:
  - 'gcr.io/$PROJECT_ID/telegram-ai-processor:$BUILD_ID'
  - 'gcr.io/$PROJECT_ID/telegram-ai-processor:latest'
