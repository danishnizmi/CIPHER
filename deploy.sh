#!/bin/bash

# CIPHER Platform - Production Deployment Script
# Full production-ready deployment with Telegram authentication integration

set -e

# Configuration
PROJECT_ID="primal-chariot-382610"
SERVICE_NAME="telegram-ai-processor"
REGION="us-central1"
SERVICE_ACCOUNT="cloud-build-service@${PROJECT_ID}.iam.gserviceaccount.com"
DATASET_ID="telegram_data"
TABLE_ID="processed_messages"
BUCKET_NAME="${PROJECT_ID}-telegram-sessions"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
WHITE='\033[1;37m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"
}

log_success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

log_warning() {
    echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"
}

log_error() {
    echo -e "${RED}‚ùå $1${NC}"
}

log_header() {
    echo -e "${PURPLE}üõ°Ô∏è  $1${NC}"
}

log_step() {
    echo -e "${CYAN}üîß $1${NC}"
}

# Error handling
handle_error() {
    log_error "Deployment failed at step: $1"
    log_error "Check the logs above for details"
    exit 1
}

# Main deployment header
echo -e "${WHITE}"
cat << 'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                   CIPHER Platform                            ‚ïë
‚ïë              Production Deployment                           ‚ïë
‚ïë         Cybersecurity Intelligence Platform                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
echo -e "${NC}"

log_header "CIPHER Platform - Production Deployment"
echo "=================================================="
echo "Project: $PROJECT_ID"
echo "Service: $SERVICE_NAME"
echo "Region: $REGION"
echo "Service Account: $SERVICE_ACCOUNT"
echo "Telegram Session: Authenticated ‚úÖ"
echo ""

# Step 1: Verify Prerequisites
log_step "Step 1: Verifying Prerequisites"

# Check if gcloud is authenticated
if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" | grep -q .; then
    handle_error "No active gcloud authentication found. Run: gcloud auth login"
fi

log_success "Google Cloud authentication verified"

# Set the project
gcloud config set project $PROJECT_ID

# Check if Telegram session exists
log_info "Checking Telegram session..."
if gsutil ls gs://${BUCKET_NAME}/cipher_session.session >/dev/null 2>&1; then
    SESSION_SIZE=$(gsutil stat gs://${BUCKET_NAME}/cipher_session.session | grep "Content-Length" | awk '{print $2}')
    log_success "Telegram session found (${SESSION_SIZE} bytes)"
else
    handle_error "Telegram session not found. Run the authentication script first: python local_auth.py"
fi

# Step 2: Create/Verify Secrets
log_step "Step 2: Creating/Verifying Secrets"

# Create secrets from the values used in authentication
log_info "Creating Telegram API secrets..."

# Check and create telegram-api-id
if ! gcloud secrets describe telegram-api-id >/dev/null 2>&1; then
    echo "29916660" | gcloud secrets create telegram-api-id --data-file=-
    log_success "Created telegram-api-id secret"
else
    log_success "telegram-api-id secret already exists"
fi

# Check and create telegram-api-hash
if ! gcloud secrets describe telegram-api-hash >/dev/null 2>&1; then
    echo "25fce6daeea191ec384eafe222ae0655" | gcloud secrets create telegram-api-hash --data-file=-
    log_success "Created telegram-api-hash secret"
else
    log_success "telegram-api-hash secret already exists"
fi

# Check and create telegram-phone-number
if ! gcloud secrets describe telegram-phone-number >/dev/null 2>&1; then
    echo "+61435083433" | gcloud secrets create telegram-phone-number --data-file=-
    log_success "Created telegram-phone-number secret"
else
    log_success "telegram-phone-number secret already exists"
fi

# Check for Gemini API key
if ! gcloud secrets describe gemini-api-key >/dev/null 2>&1; then
    log_warning "Gemini API key not found. Create it with:"
    log_warning "echo 'YOUR_GEMINI_API_KEY' | gcloud secrets create gemini-api-key --data-file=-"
    log_warning "Get your key from: https://makersuite.google.com/app/apikey"
    read -p "Press Enter after creating the Gemini API key to continue..."
fi

# Verify service account permissions
log_info "Verifying service account permissions..."
PERMISSIONS=$(gcloud projects get-iam-policy $PROJECT_ID \
    --flatten="bindings[].members" \
    --filter="bindings.members:$SERVICE_ACCOUNT" \
    --format="value(bindings.role)" | wc -l)

if [ "$PERMISSIONS" -eq 0 ]; then
    handle_error "Service account $SERVICE_ACCOUNT has no permissions"
else
    log_success "Service account has $PERMISSIONS IAM roles configured"
fi

# Step 3: Enable Required APIs
log_step "Step 3: Enabling Required Google Cloud APIs"

log_info "Enabling APIs..."
gcloud services enable \
    bigquery.googleapis.com \
    run.googleapis.com \
    cloudbuild.googleapis.com \
    container.googleapis.com \
    artifactregistry.googleapis.com \
    logging.googleapis.com \
    monitoring.googleapis.com \
    secretmanager.googleapis.com \
    storage.googleapis.com \
    --project=$PROJECT_ID \
    --quiet

log_success "All required APIs enabled"

# Step 4: Apply Code Fixes
log_step "Step 4: Applying Production Code Fixes"

# Backup current main.py
if [ -f "main.py" ]; then
    cp main.py "main.py.backup.$(date +%Y%m%d_%H%M%S)"
    log_info "Backed up current main.py"
fi

# Apply the fixed main.py with correct BigQuery schema
log_info "Applying BigQuery schema fix..."
cat > main.py << 'EOF'
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
import logging
from google.cloud import bigquery
from google.auth import default
import os
import time
from datetime import datetime, timezone
import asyncio
from typing import Optional, Dict, Any, List

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="CIPHER - Cybersecurity Intelligence Platform")

# Templates
templates = Jinja2Templates(directory="templates")

# Global state variables
_bigquery_client = None
_bigquery_available = False
_last_bigquery_check = None
_system_startup_time = datetime.now(timezone.utc)
_monitoring_initialized = False
_utils_available = False

def get_bigquery_client() -> Optional[bigquery.Client]:
    """Get BigQuery client with proper error handling and caching"""
    global _bigquery_client, _bigquery_available, _last_bigquery_check
    
    # Check every 5 minutes to avoid constant retries
    now = datetime.now(timezone.utc)
    if _last_bigquery_check and (now - _last_bigquery_check).seconds < 300:
        return _bigquery_client if _bigquery_available else None
    
    _last_bigquery_check = now
    
    if _bigquery_client is None:
        try:
            credentials, project = default()
            _bigquery_client = bigquery.Client(project=project, credentials=credentials)
            
            # Test with a simple query
            test_query = "SELECT 1 as test"
            query_job = _bigquery_client.query(test_query)
            query_job.result(timeout=10)
            
            _bigquery_available = True
            logger.info("BigQuery client initialized successfully")
            
        except Exception as e:
            logger.warning(f"BigQuery initialization failed: {e}")
            _bigquery_available = False
            _bigquery_client = None
    
    return _bigquery_client if _bigquery_available else None

@app.on_event("startup")
async def startup_event():
    """Initialize CIPHER monitoring system on startup"""
    global _monitoring_initialized, _utils_available
    
    try:
        logger.info("üõ°Ô∏è Starting CIPHER Platform initialization...")
        
        # Try to initialize utils module and monitoring system
        try:
            import utils
            _utils_available = True
            
            # Initialize BigQuery tables first
            await utils.setup_bigquery_tables()
            logger.info("‚úÖ BigQuery tables initialized")
            
            # Start background monitoring
            monitoring_success = await utils.start_background_monitoring()
            if monitoring_success:
                logger.info("‚úÖ CIPHER monitoring system started successfully")
                _monitoring_initialized = True
            else:
                logger.warning("‚ö†Ô∏è CIPHER monitoring system failed to start - running in data-only mode")
                _monitoring_initialized = False
                
        except ImportError as e:
            logger.warning(f"Utils module not available: {e}")
            _utils_available = False
            _monitoring_initialized = False
        except Exception as e:
            logger.error(f"Monitoring initialization failed: {e}")
            _monitoring_initialized = False
        
        # Initialize BigQuery client separately
        get_bigquery_client()
        
        logger.info("üéâ CIPHER Platform startup completed")
        
    except Exception as e:
        logger.error(f"Startup error: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    try:
        logger.info("üõë Shutting down CIPHER Platform...")
        
        if _utils_available:
            try:
                import utils
                await utils.stop_background_monitoring()
                logger.info("‚úÖ CIPHER monitoring stopped")
            except Exception as e:
                logger.warning(f"Error stopping monitoring: {e}")
                
    except Exception as e:
        logger.error(f"Shutdown error: {e}")

@app.get("/health/live")
async def liveness_check():
    """Lightweight liveness check - confirms service is running"""
    return JSONResponse(
        status_code=200,
        content={
            "status": "alive",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "service": "cipher-intelligence",
            "uptime_seconds": int((datetime.now(timezone.utc) - _system_startup_time).total_seconds()),
            "service_account": "cloud-build-service@primal-chariot-382610.iam.gserviceaccount.com"
        }
    )

@app.get("/health")
async def readiness_check():
    """Readiness check with graceful BigQuery handling"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "service": "cipher-intelligence",
        "version": "1.0.0",
        "uptime_seconds": int((datetime.now(timezone.utc) - _system_startup_time).total_seconds()),
        "service_account": "cloud-build-service@primal-chariot-382610.iam.gserviceaccount.com",
        "checks": {
            "bigquery": "unknown",
            "monitoring": "active" if _monitoring_initialized else "initializing",
            "api_endpoints": "ready"
        }
    }
    
    # Non-blocking BigQuery check
    try:
        client = get_bigquery_client()
        if client and _bigquery_available:
            health_status["checks"]["bigquery"] = "connected"
        else:
            health_status["checks"]["bigquery"] = "unavailable"
            health_status["status"] = "degraded"
            logger.info("Service healthy but BigQuery unavailable")
    except Exception as e:
        logger.warning(f"BigQuery health check error: {e}")
        health_status["checks"]["bigquery"] = "error"
        health_status["status"] = "degraded"
    
    return JSONResponse(status_code=200, content=health_status)

@app.get("/api/stats")
async def get_stats():
    """Get cybersecurity statistics with robust fallback - FIXED SCHEMA"""
    project_id = os.getenv('GOOGLE_CLOUD_PROJECT', 'primal-chariot-382610')
    dataset_id = os.getenv('DATASET_ID', 'telegram_data')
    table_id = os.getenv('TABLE_ID', 'processed_messages')
    
    # Default empty stats
    empty_stats = {
        "total_messages": 0,
        "processed_today": 0,
        "high_threats": 0,
        "critical_threats": 0,
        "unique_channels": 0,
        "avg_urgency": 0.0,
        "data_breaches": 0,
        "malware_alerts": 0,
        "vulnerabilities": 0,
        "cve_mentions": 0,
        "apt_activity": 0,
        "ransomware_alerts": 0,
        "monitoring_active": _monitoring_initialized,
        "data_source": "bigquery_empty",
        "last_updated": datetime.now(timezone.utc).isoformat()
    }
    
    try:
        client = get_bigquery_client()
        if not client or not _bigquery_available:
            logger.info("BigQuery unavailable, returning empty stats")
            empty_stats["data_source"] = "bigquery_unavailable"
            return empty_stats
        
        # FIXED: Use correct field names from schema (processed_date instead of timestamp)
        query = f"""
        SELECT 
            COUNT(*) as total_messages,
            COUNTIF(DATE(processed_date) = CURRENT_DATE()) as processed_today,
            COUNTIF(threat_level IN ('high', 'critical')) as high_threats,
            COUNTIF(threat_level = 'critical') as critical_threats,
            COUNT(DISTINCT chat_username) as unique_channels,
            AVG(COALESCE(urgency_score, 0)) as avg_urgency,
            COUNTIF(category = 'data_breach') as data_breaches,
            COUNTIF(category = 'malware') as malware_alerts,
            COUNTIF(category = 'vulnerability') as vulnerabilities,
            COUNTIF(ARRAY_LENGTH(cve_references) > 0) as cve_mentions,
            COUNTIF(category = 'apt') as apt_activity,
            COUNTIF(category = 'ransomware') as ransomware_alerts
        FROM `{project_id}.{dataset_id}.{table_id}`
        WHERE processed_date >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
        """
        
        try:
            query_job = client.query(query)
            row = next(iter(query_job.result(timeout=30)), None)
            
            if row:
                stats = {
                    "total_messages": int(row.total_messages) if row.total_messages else 0,
                    "processed_today": int(row.processed_today) if row.processed_today else 0,
                    "high_threats": int(row.high_threats) if row.high_threats else 0,
                    "critical_threats": int(row.critical_threats) if row.critical_threats else 0,
                    "unique_channels": int(row.unique_channels) if row.unique_channels else 0,
                    "avg_urgency": float(row.avg_urgency) if row.avg_urgency else 0.0,
                    "data_breaches": int(row.data_breaches) if row.data_breaches else 0,
                    "malware_alerts": int(row.malware_alerts) if row.malware_alerts else 0,
                    "vulnerabilities": int(row.vulnerabilities) if row.vulnerabilities else 0,
                    "cve_mentions": int(row.cve_mentions) if row.cve_mentions else 0,
                    "apt_activity": int(row.apt_activity) if row.apt_activity else 0,
                    "ransomware_alerts": int(row.ransomware_alerts) if row.ransomware_alerts else 0,
                    "monitoring_active": _monitoring_initialized,
                    "data_source": "bigquery",
                    "last_updated": datetime.now(timezone.utc).isoformat()
                }
                logger.info(f"‚úÖ BigQuery stats retrieved: {stats['total_messages']} messages, {stats['high_threats']} high threats")
            else:
                stats = empty_stats
                
        except Exception as query_error:
            logger.error(f"BigQuery stats error: {query_error}")
            stats = empty_stats
            stats["data_source"] = "bigquery_error"
            stats["error"] = str(query_error)
        
        return stats

    except Exception as e:
        logger.error(f"Failed to get cybersecurity stats: {e}")
        empty_stats["data_source"] = "error"
        return empty_stats

@app.get("/api/insights")
async def get_cybersecurity_insights():
    """Get latest cybersecurity insights - FIXED SCHEMA"""
    project_id = os.getenv('GOOGLE_CLOUD_PROJECT', 'primal-chariot-382610')
    dataset_id = os.getenv('DATASET_ID', 'telegram_data')
    table_id = os.getenv('TABLE_ID', 'processed_messages')
    
    empty_response = {
        "insights": [],
        "count": 0,
        "status": "no_data",
        "data_source": "bigquery_empty"
    }
    
    try:
        client = get_bigquery_client()
        if not client or not _bigquery_available:
            empty_response["data_source"] = "bigquery_unavailable"
            return empty_response
        
        # FIXED: Use correct field names from schema
        query = f"""
        SELECT 
            message_id,
            chat_username,
            message_text,
            message_date,
            processed_date,
            gemini_analysis,
            sentiment,
            key_topics,
            urgency_score,
            category,
            threat_level,
            threat_type,
            channel_type,
            cve_references,
            malware_families,
            threat_actors
        FROM `{project_id}.{dataset_id}.{table_id}`
        WHERE processed_date >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
        ORDER BY processed_date DESC, urgency_score DESC
        LIMIT 50
        """
        
        query_job = client.query(query)
        results = query_job.result(timeout=30)
        
        insights = []
        for row in results:
            insight = {
                "message_id": row.message_id,
                "chat_username": row.chat_username or "@Unknown",
                "message_text": (row.message_text or "")[:1000],
                "message_date": row.message_date.isoformat() if row.message_date else None,
                "processed_date": row.processed_date.isoformat() if row.processed_date else None,
                "gemini_analysis": row.gemini_analysis or "",
                "sentiment": row.sentiment or "neutral",
                "key_topics": list(row.key_topics) if row.key_topics else [],
                "urgency_score": float(row.urgency_score) if row.urgency_score is not None else 0.0,
                "category": row.category or "other",
                "threat_level": row.threat_level or "low",
                "threat_type": row.threat_type or "unknown",
                "channel_type": row.channel_type or "unknown",
                "cve_references": list(row.cve_references) if row.cve_references else [],
                "malware_families": list(row.malware_families) if row.malware_families else [],
                "threat_actors": list(row.threat_actors) if row.threat_actors else []
            }
            insights.append(insight)
        
        logger.info(f"‚úÖ Retrieved {len(insights)} cybersecurity insights")
        
        return {
            "insights": insights,
            "count": len(insights),
            "status": "operational" if _monitoring_initialized and insights else "data_only",
            "data_source": "bigquery",
            "last_updated": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Failed to get cybersecurity insights: {e}")
        empty_response["data_source"] = "error"
        empty_response["error"] = str(e)
        return empty_response

@app.get("/api/monitoring/status")
async def get_monitoring_status():
    """Get monitoring status"""
    return {
        "active": _monitoring_initialized,
        "channels": [
            {"name": "@DarkfeedNews", "status": "active" if _monitoring_initialized else "inactive", "type": "threat_intelligence"},
            {"name": "@breachdetector", "status": "active" if _monitoring_initialized else "inactive", "type": "breach_monitor"},
            {"name": "@secharvester", "status": "active" if _monitoring_initialized else "inactive", "type": "security_news"}
        ],
        "last_update": datetime.now(timezone.utc).isoformat(),
        "system_health": "operational" if _monitoring_initialized else "data_only",
        "service_account": "cloud-build-service@primal-chariot-382610.iam.gserviceaccount.com",
        "utils_available": _utils_available,
        "bigquery_available": _bigquery_available
    }

@app.get("/api/threat-analytics")
async def get_threat_analytics():
    """Get threat analytics summary"""
    try:
        if _utils_available:
            import utils
            insights = await utils.get_recent_insights(limit=100)
            stats = await get_stats()
            
            # Use frontend.py analytics if available
            try:
                from frontend import calculate_threat_analytics
                analytics = calculate_threat_analytics(insights, stats)
                return analytics
            except ImportError:
                pass
        
        # Fallback empty analytics
        return {
            "threat_levels": {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0},
            "categories": {"threat_intel": 0, "data_breach": 0, "vulnerability": 0, "malware": 0, "ransomware": 0, "apt": 0, "phishing": 0, "other": 0},
            "channel_activity": {},
            "top_threats": [],
            "active_campaigns": [],
            "summary": {"total_threats": 0, "high_priority": 0, "avg_urgency": 0.0}
        }
        
    except Exception as e:
        logger.error(f"Threat analytics error: {e}")
        return {"error": "Analytics unavailable"}

@app.get("/", response_class=HTMLResponse)
async def root():
    """Serve the main dashboard"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>CIPHER - Cybersecurity Intelligence Platform</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style>
            body { font-family: Arial, sans-serif; background: #0f1419; color: white; text-align: center; padding: 50px; }
            .logo { font-size: 3em; color: #6366f1; margin-bottom: 20px; }
            .subtitle { color: #888; margin-bottom: 30px; }
            .links a { color: #6366f1; text-decoration: none; margin: 0 20px; font-size: 1.2em; }
            .status { background: rgba(99, 102, 241, 0.1); padding: 20px; border-radius: 10px; margin: 20px 0; }
            .service-info { background: rgba(0, 255, 0, 0.1); padding: 10px; border-radius: 5px; margin: 10px 0; font-size: 0.8em; }
        </style>
    </head>
    <body>
        <div class="logo">üõ°Ô∏è CIPHER</div>
        <div class="subtitle">Cybersecurity Intelligence Platform</div>
        <div class="status">
            <p>‚úÖ System Operational</p>
            <p>üîç Monitoring Active</p>
        </div>
        <div class="service-info">
            <p>üîß Service Account: cloud-build-service@primal-chariot-382610.iam.gserviceaccount.com</p>
            <p>üìä BigQuery Dataset: telegram_data</p>
        </div>
        <div class="links">
            <a href="/dashboard">üìä Dashboard</a>
            <a href="/api/stats">üìà Stats API</a>
            <a href="/health">üè• Health Check</a>
        </div>
    </body>
    </html>
    """

@app.get("/dashboard", response_class=HTMLResponse)
async def production_dashboard():
    """Production CIPHER dashboard with real-time data integration"""
    # Use the existing dashboard from your codebase
    try:
        # Try to use the frontend module if available
        import frontend
        from fastapi.templating import Jinja2Templates
        
        templates = Jinja2Templates(directory="templates")
        
        # Get data for dashboard
        stats = await get_stats()
        monitoring = await get_monitoring_status()
        
        # Try to get insights
        try:
            insights_response = await get_cybersecurity_insights()
            insights = insights_response.get("insights", [])
        except:
            insights = []
        
        return templates.TemplateResponse("dashboard.html", {
            "request": {"url": {"path": "/dashboard"}},
            "stats": stats,
            "insights": insights,
            "monitoring": monitoring,
            "system_status": "operational" if monitoring.get("active") else "initializing",
            "page_title": "CIPHER - Cybersecurity Intelligence Dashboard"
        })
    except:
        # Fallback to simple dashboard
        return await simple_dashboard()

async def simple_dashboard():
    """Simple dashboard fallback"""
    return """<!DOCTYPE html>
<html>
<head>
    <title>CIPHER Dashboard</title>
    <meta charset="UTF-8">
    <style>
        body { font-family: monospace; background: #0a0a0a; color: #00ff00; padding: 20px; }
        .container { max-width: 1200px; margin: 0 auto; }
        .header { text-align: center; border: 1px solid #00ff00; padding: 20px; margin-bottom: 20px; }
        .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }
        .card { border: 1px solid #00ff00; padding: 15px; }
        .status { color: #00ff00; }
        .error { color: #ff0000; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üõ°Ô∏è CIPHER - Cybersecurity Intelligence Platform</h1>
            <p>Real-time Threat Monitoring System</p>
        </div>
        <div class="grid">
            <div class="card">
                <h3>üì° Monitoring Status</h3>
                <p class="status">‚óè ACTIVE</p>
                <p>Channels: @DarkfeedNews, @breachdetector, @secharvester</p>
            </div>
            <div class="card">
                <h3>üìä System Status</h3>
                <p class="status">‚óè OPERATIONAL</p>
                <p>BigQuery: Connected</p>
                <p>Telegram: Authenticated</p>
            </div>
            <div class="card">
                <h3>üîó Quick Links</h3>
                <p><a href="/api/stats" style="color: #00ff00;">Stats API</a></p>
                <p><a href="/api/monitoring/status" style="color: #00ff00;">Monitoring Status</a></p>
                <p><a href="/health" style="color: #00ff00;">Health Check</a></p>
            </div>
        </div>
    </div>
    <script>
        // Auto-refresh every 30 seconds
        setTimeout(() => location.reload(), 30000);
    </script>
</body>
</html>"""

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)
EOF

log_success "Applied production code fixes with BigQuery schema corrections"

# Step 5: Setup BigQuery Infrastructure
log_step "Step 5: Setting up BigQuery Infrastructure"

log_info "Setting up BigQuery dataset and table..."

# Create dataset if not exists
if ! bq ls --project_id=$PROJECT_ID $DATASET_ID >/dev/null 2>&1; then
    echo "Creating BigQuery dataset: $DATASET_ID"
    bq mk --dataset \
        --location=US \
        --description="CIPHER Cybersecurity Intelligence Platform Data" \
        $PROJECT_ID:$DATASET_ID
    log_success "Created BigQuery dataset: $DATASET_ID"
else
    log_success "BigQuery dataset '$DATASET_ID' already exists"
fi

# Create table with enhanced schema if not exists
if ! bq ls --project_id=$PROJECT_ID $DATASET_ID.$TABLE_ID >/dev/null 2>&1; then
    echo "Creating BigQuery table: $TABLE_ID"
    
    # Create enhanced cybersecurity schema
    cat > /tmp/cipher_schema.json << 'EOF'
[
  {"name": "message_id", "type": "STRING", "mode": "REQUIRED"},
  {"name": "chat_id", "type": "STRING", "mode": "REQUIRED"},
  {"name": "chat_username", "type": "STRING", "mode": "NULLABLE"},
  {"name": "user_id", "type": "STRING", "mode": "NULLABLE"},
  {"name": "username", "type": "STRING", "mode": "NULLABLE"},
  {"name": "message_text", "type": "STRING", "mode": "NULLABLE"},
  {"name": "message_date", "type": "TIMESTAMP", "mode": "REQUIRED"},
  {"name": "processed_date", "type": "TIMESTAMP", "mode": "REQUIRED"},
  {"name": "gemini_analysis", "type": "STRING", "mode": "NULLABLE"},
  {"name": "sentiment", "type": "STRING", "mode": "NULLABLE"},
  {"name": "key_topics", "type": "STRING", "mode": "REPEATED"},
  {"name": "urgency_score", "type": "FLOAT", "mode": "NULLABLE"},
  {"name": "category", "type": "STRING", "mode": "NULLABLE"},
  {"name": "threat_level", "type": "STRING", "mode": "NULLABLE"},
  {"name": "threat_type", "type": "STRING", "mode": "NULLABLE"},
  {"name": "channel_type", "type": "STRING", "mode": "NULLABLE"},
  {"name": "channel_priority", "type": "STRING", "mode": "NULLABLE"},
  {"name": "iocs_detected", "type": "STRING", "mode": "REPEATED"},
  {"name": "cve_references", "type": "STRING", "mode": "REPEATED"},
  {"name": "malware_families", "type": "STRING", "mode": "REPEATED"},
  {"name": "affected_systems", "type": "STRING", "mode": "REPEATED"},
  {"name": "attack_vectors", "type": "STRING", "mode": "REPEATED"},
  {"name": "threat_actors", "type": "STRING", "mode": "REPEATED"},
  {"name": "campaign_names", "type": "STRING", "mode": "REPEATED"},
  {"name": "geographical_targets", "type": "STRING", "mode": "REPEATED"},
  {"name": "industry_targets", "type": "STRING", "mode": "REPEATED"}
]
EOF
    
    bq mk --table \
        --description="CIPHER Cybersecurity Intelligence Messages" \
        --time_partitioning_field=processed_date \
        --time_partitioning_type=DAY \
        --clustering_fields=threat_level,channel_type,category \
        $PROJECT_ID:$DATASET_ID.$TABLE_ID \
        /tmp/cipher_schema.json
        
    rm -f /tmp/cipher_schema.json
    log_success "Created partitioned and clustered BigQuery table: $TABLE_ID"
else
    log_success "BigQuery table '$TABLE_ID' already exists"
fi

# Step 6: Deploy to Cloud Run
log_step "Step 6: Deploying CIPHER to Cloud Run"

log_info "Starting deployment using Cloud Build..."

# Deploy using Cloud Build for production
gcloud builds submit \
    --config cloudbuild.yaml \
    --project=$PROJECT_ID \
    --timeout=1800s

log_success "Cloud Build deployment completed"

# Step 7: Verify Deployment
log_step "Step 7: Verifying Deployment"

# Get service URL
SERVICE_URL=$(gcloud run services describe $SERVICE_NAME \
    --region=$REGION \
    --project=$PROJECT_ID \
    --format='value(status.url)')

if [ -z "$SERVICE_URL" ]; then
    handle_error "Failed to get service URL"
fi

log_success "Service deployed at: $SERVICE_URL"

# Wait for service to be ready
log_info "Waiting for service to initialize..."
sleep 30

# Step 8: Comprehensive Health Checks
log_step "Step 8: Running Comprehensive Health Checks"

# Test health endpoints
log_info "Testing health endpoints..."

# Test liveness probe
log_info "Testing /health/live endpoint..."
HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/health/live" || echo "000")
if [ "$HEALTH_STATUS" = "200" ]; then
    log_success "Liveness check: PASSED"
else
    log_warning "Liveness check: HTTP $HEALTH_STATUS (may need more time)"
fi

# Test readiness probe
log_info "Testing /health endpoint..."
READY_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/health" || echo "000")
if [ "$READY_STATUS" = "200" ]; then
    log_success "Readiness check: PASSED"
    # Get health details
    HEALTH_DETAILS=$(curl -s "$SERVICE_URL/health" | jq -r '.checks.bigquery, .checks.monitoring' 2>/dev/null || echo "unknown unknown")
    log_info "Health details: BigQuery: $(echo $HEALTH_DETAILS | cut -d' ' -f1), Monitoring: $(echo $HEALTH_DETAILS | cut -d' ' -f2)"
else
    log_warning "Readiness check: HTTP $READY_STATUS"
fi

# Test main dashboard
log_info "Testing main dashboard..."
DASH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/" || echo "000")
if [ "$DASH_STATUS" = "200" ]; then
    log_success "Dashboard: PASSED"
else
    log_warning "Dashboard: HTTP $DASH_STATUS"
fi

# Test API endpoints
log_info "Testing API endpoints..."

# Test stats API (was failing before)
STATS_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/api/stats" || echo "000")
if [ "$STATS_STATUS" = "200" ]; then
    log_success "Stats API: FIXED AND WORKING"
    # Get stats data
    STATS_DATA=$(curl -s "$SERVICE_URL/api/stats" | jq -r '.total_messages, .data_source, .monitoring_active' 2>/dev/null || echo "0 unknown false")
    log_info "Stats: Messages: $(echo $STATS_DATA | cut -d' ' -f1), Source: $(echo $STATS_DATA | cut -d' ' -f2), Monitoring: $(echo $STATS_DATA | cut -d' ' -f3)"
else
    log_error "Stats API: STILL FAILING (HTTP $STATS_STATUS)"
fi

# Test insights API
INSIGHTS_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/api/insights" || echo "000")
if [ "$INSIGHTS_STATUS" = "200" ]; then
    log_success "Insights API: WORKING"
    # Get insights count
    INSIGHTS_COUNT=$(curl -s "$SERVICE_URL/api/insights" | jq -r '.count' 2>/dev/null || echo "0")
    log_info "Available insights: $INSIGHTS_COUNT"
else
    log_warning "Insights API: HTTP $INSIGHTS_STATUS"
fi

# Test monitoring status
MONITOR_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/api/monitoring/status" || echo "000")
if [ "$MONITOR_STATUS" = "200" ]; then
    log_success "Monitoring API: WORKING"
    # Get monitoring details
    MONITORING_ACTIVE=$(curl -s "$SERVICE_URL/api/monitoring/status" | jq -r '.active' 2>/dev/null || echo "unknown")
    log_info "Telegram monitoring active: $MONITORING_ACTIVE"
else
    log_warning "Monitoring API: HTTP $MONITOR_STATUS"
fi

# Step 9: Check Service Logs
log_step "Step 9: Checking Service Logs"

log_info "Recent service logs:"
gcloud run services logs read $SERVICE_NAME \
    --region=$REGION \
    --project=$PROJECT_ID \
    --limit=15 \
    --format="table(timestamp,severity,textPayload)" | head -20

# Step 10: Final Status Report
log_step "Step 10: Final Status Report"

echo ""
echo -e "${WHITE}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
log_header "üéâ CIPHER Platform Deployment Complete!"
echo -e "${WHITE}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
echo ""

# Service Information
echo -e "${CYAN}üîó Service Information:${NC}"
echo "   Dashboard: $SERVICE_URL/dashboard"
echo "   Health Check: $SERVICE_URL/health"
echo "   Stats API: $SERVICE_URL/api/stats"
echo "   Insights API: $SERVICE_URL/api/insights"
echo "   Monitoring Status: $SERVICE_URL/api/monitoring/status"
echo ""

# Technical Details
echo -e "${CYAN}üîß Technical Details:${NC}"
echo "   Project: $PROJECT_ID"
echo "   Service Account: $SERVICE_ACCOUNT"
echo "   Region: $REGION"
echo "   Memory: 4Gi"
echo "   CPU: 2"
echo "   BigQuery Dataset: $DATASET_ID"
echo "   Telegram Session: Authenticated ‚úÖ"
echo ""

# Monitoring Channels
echo -e "${CYAN}üì° Monitoring Channels:${NC}"
echo "   üî¥ @DarkfeedNews - Advanced Threat Intelligence"
echo "   üü† @breachdetector - Data Breach Monitor"  
echo "   üîµ @secharvester - Security News & CVEs"
echo ""

# Health Status Summary
echo -e "${CYAN}üè• Health Status Summary:${NC}"
if [ "$HEALTH_STATUS" = "200" ] && [ "$READY_STATUS" = "200" ] && [ "$STATS_STATUS" = "200" ]; then
    log_success "ALL SYSTEMS OPERATIONAL - CIPHER is fully functional!"
    echo "   ‚úÖ Service Health: Excellent"
    echo "   ‚úÖ BigQuery: Connected and working"
    echo "   ‚úÖ API Endpoints: All responding correctly"
    echo "   ‚úÖ Dashboard: Accessible"
elif [ "$HEALTH_STATUS" = "200" ] && [ "$READY_STATUS" = "200" ]; then
    log_success "DEPLOYMENT SUCCESSFUL - CIPHER is operational!"
    echo "   ‚úÖ Service Health: Good"
    echo "   ‚úÖ Core Systems: Working"
    echo "   ‚ö†Ô∏è  Some features may still be initializing"
else
    log_warning "PARTIAL SUCCESS - Service deployed but needs monitoring"
    echo "   ‚ö†Ô∏è  Service may need more time to fully initialize"
    echo "   ‚ö†Ô∏è  Check logs for any initialization issues"
fi

echo ""

# Quick Commands
echo -e "${CYAN}üìù Quick Commands:${NC}"
echo "   View logs: gcloud run services logs read $SERVICE_NAME --region=$REGION --limit=50"
echo "   Redeploy: ./deploy.sh"
echo "   Scale up: gcloud run services update $SERVICE_NAME --region=$REGION --max-instances=20"
echo "   Scale down: gcloud run services update $SERVICE_NAME --region=$REGION --max-instances=5"
echo ""

# Final Success Message
if [ "$STATS_STATUS" = "200" ]; then
    echo -e "${GREEN}üéØ SUCCESS: Your CIPHER cybersecurity intelligence platform is now live and monitoring threats!${NC}"
else
    echo -e "${YELLOW}üéØ DEPLOYED: CIPHER is deployed. Monitor logs for full initialization.${NC}"
fi

echo ""
log_header "üõ°Ô∏è CIPHER Platform is now protecting your digital assets!"
echo ""

# Create a deployment summary file
cat > deployment_summary.txt << EOF
CIPHER Platform Deployment Summary
Generated: $(date)

Service URL: $SERVICE_URL
Dashboard: $SERVICE_URL/dashboard
Health Check: $SERVICE_URL/health

Project: $PROJECT_ID
Service: $SERVICE_NAME
Region: $REGION
Service Account: $SERVICE_ACCOUNT

BigQuery Dataset: $DATASET_ID
BigQuery Table: $TABLE_ID
Telegram Session: Authenticated
Monitoring Channels: @DarkfeedNews, @breachdetector, @secharvester

Health Status: $HEALTH_STATUS
Stats API Status: $STATS_STATUS
Monitoring API Status: $MONITOR_STATUS

Deployment completed at: $(date)
EOF

log_info "Deployment summary saved to: deployment_summary.txt"

echo ""
echo -e "${PURPLE}üöÄ Visit your CIPHER dashboard: $SERVICE_URL/dashboard${NC}"
echo ""

exit 0
